{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_questions = json.load(open('autocast_questions.json')) # from the Autocast dataset\n",
    "test_questions = json.load(open('autocast_competition_test_set.json'))\n",
    "test_ids = [q['id'] for q in test_questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create baseline models outputting random answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_baseline_model(question):\n",
    "#     if question['qtype'] == 't/f':\n",
    "#         return np.random.random(size=2)\n",
    "#     elif question['qtype'] == 'mc':\n",
    "#         probs = np.random.random(size=len(question['choices']))\n",
    "#         return probs / probs.sum()\n",
    "#     elif question['qtype'] == 'num':\n",
    "#         return np.random.random()\n",
    "\n",
    "\n",
    "# def calibrated_random_baseline_model(question):\n",
    "#     if question['qtype'] == 't/f':\n",
    "#         pred_idx = np.argmax(np.random.random(size=2))\n",
    "#         pred = np.ones(2)\n",
    "#         pred[pred_idx] += 1e-5\n",
    "#         return pred / pred.sum()\n",
    "#     elif question['qtype'] == 'mc':\n",
    "#         pred_idx = np.argmax(np.random.random(size=len(question['choices'])))\n",
    "#         pred = np.ones(len(question['choices']))\n",
    "#         pred[pred_idx] += 1e-5\n",
    "#         return pred / pred.sum()\n",
    "#     elif question['qtype'] == 'num':\n",
    "#         return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T/F: 29.54, MCQ: 49.55, NUM: 22.63\n",
      "Combined Metric: 101.71\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the device to use\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the Autocast and competition test set questions\n",
    "autocast_questions = json.load(open('autocast_questions.json'))\n",
    "test_questions = json.load(open('autocast_competition_test_set.json'))\n",
    "test_ids = [q['id'] for q in test_questions]\n",
    "\n",
    "def bert_model(question):\n",
    "    # Tokenize the question and convert to input features\n",
    "    question_text = question['question']\n",
    "    if question['qtype'] == 't/f':\n",
    "        choices_text = ['true', 'false']\n",
    "    elif question['qtype'] == 'mc':\n",
    "        choices_text = question['choices']\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "    inputs = tokenizer(question_text, choices_text, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Make a forward pass through the BERT model and get the logits\n",
    "    logits = model(**inputs).logits.squeeze()\n",
    "\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "\n",
    "    # Return the probabilities\n",
    "    return probs.tolist()\n",
    "\n",
    "def brier_score(probabilities, answer_probabilities):\n",
    "    if len(probabilities) != len(answer_probabilities):\n",
    "        return 0.5  # return a neutral score for invalid predictions\n",
    "    return ((probabilities - answer_probabilities) ** 2).sum() / 2\n",
    "\n",
    "\n",
    "preds = []\n",
    "answers = []\n",
    "qtypes = []\n",
    "\n",
    "for question in autocast_questions:\n",
    "    if question['id'] in test_ids: # skipping questions in the competition test set\n",
    "        continue\n",
    "    if question['answer'] is None: # skipping questions without answer\n",
    "        continue\n",
    "    preds.append(bert_model(question))\n",
    "    if question['qtype'] == 't/f':\n",
    "        ans_idx = 0 if question['answer'] == 'no' else 1\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        qtypes.append('t/f')\n",
    "    elif question['qtype'] == 'mc':\n",
    "        ans_idx = ord(question['answer']) - ord('A')\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        qtypes.append('mc')\n",
    "    elif question['qtype'] == 'num':\n",
    "        ans = float(question['answer'])\n",
    "        qtypes.append('num')\n",
    "    answers.append(ans)\n",
    "\n",
    "tf_results, mc_results, num_results = [], [], []\n",
    "\n",
    "for p, a, qtype in zip(preds, answers, qtypes):\n",
    "    if qtype == 't/f':\n",
    "        tf_results.append(brier_score(p, a))\n",
    "    elif qtype == 'mc':\n",
    "        mc_results.append(brier_score(p, a))\n",
    "    else:\n",
    "        num_results.append(np.abs(p - a))\n",
    "\n",
    "print(f\"T/F: {np.mean(tf_results)*100:.2f}, MCQ: {np.mean(mc_results)*100:.2f}, NUM: {np.mean(num_results)*100:.2f}\")\n",
    "print(f\"Combined Metric: {(np.mean(tf_results) + np.mean(mc_results) + np.mean(num_results))*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get performance on the Autocast train set\n",
    "\n",
    "Note that the Autocast dataset contains questions in the competition test set. Those should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(probabilities, answer_probabilities):\n",
    "    if len(probabilities) != len(answer_probabilities):\n",
    "        return 0.5  # return a neutral score for invalid predictions\n",
    "    \n",
    "    # Calculate Brier score\n",
    "    return ((probabilities - answer_probabilities) ** 2).sum() / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = []\n",
    "# answers = []\n",
    "# qtypes = []\n",
    "# for question in autocast_questions:\n",
    "#     if question['id'] in test_ids: # skipping questions in the competition test set\n",
    "#         continue\n",
    "#     if question['answer'] is None: # skipping questions without answer\n",
    "#         continue\n",
    "#     preds.append(calibrated_random_baseline_model(question))\n",
    "#     if question['qtype'] == 't/f':\n",
    "#         ans_idx = 0 if question['answer'] == 'no' else 1\n",
    "#         ans = np.zeros(len(question['choices']))\n",
    "#         ans[ans_idx] = 1\n",
    "#         qtypes.append('t/f')\n",
    "#     elif question['qtype'] == 'mc':\n",
    "#         ans_idx = ord(question['answer']) - ord('A')\n",
    "#         ans = np.zeros(len(question['choices']))\n",
    "#         ans[ans_idx] = 1\n",
    "#         qtypes.append('mc')\n",
    "#     elif question['qtype'] == 'num':\n",
    "#         ans = float(question['answer'])\n",
    "#         qtypes.append('num')\n",
    "#     answers.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "answers = []\n",
    "qtypes = []\n",
    "for question in autocast_questions:\n",
    "    if question['id'] in test_ids: # skipping questions in the competition test set\n",
    "        continue\n",
    "    if question['answer'] is None: # skipping questions without answer\n",
    "        continue\n",
    "    preds.append(bert_model(question))\n",
    "    if question['qtype'] == 't/f':\n",
    "        ans_idx = 0 if question['answer'] == 'no' else 1\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        qtypes.append('t/f')\n",
    "    elif question['qtype'] == 'mc':\n",
    "        ans_idx = ord(question['answer']) - ord('A')\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        qtypes.append('mc')\n",
    "    elif question['qtype'] == 'num':\n",
    "        ans = float(question['answer'])\n",
    "        qtypes.append('num')\n",
    "    answers.append(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_results, mc_results, num_results = [],[],[]\n",
    "# for p, a, qtype in zip(preds, answers, qtypes):\n",
    "#     if qtype == 't/f':\n",
    "#         tf_results.append(brier_score(p, a))\n",
    "#     elif qtype == 'mc':\n",
    "#         mc_results.append(brier_score(p, a))\n",
    "#     else:\n",
    "#         num_results.append(np.abs(p - a))\n",
    "\n",
    "# print(f\"T/F: {np.mean(tf_results)*100:.2f}, MCQ: {np.mean(mc_results)*100:.2f}, NUM: {np.mean(num_results)*100:.2f}\")\n",
    "# print(f\"Combined Metric: {(np.mean(tf_results) + np.mean(mc_results) + np.mean(num_results))*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T/F: 29.54, MCQ: 25.45, NUM: 22.63\n",
      "Combined Metric: 77.61\n"
     ]
    }
   ],
   "source": [
    "tf_results, mc_results, num_results = [],[],[]\n",
    "for p, a, qtype in zip(preds, answers, qtypes):\n",
    "    if qtype == 't/f':\n",
    "        tf_results.append(brier_score(p, a))\n",
    "    elif qtype == 'mc':\n",
    "        if len(p) == 2:\n",
    "            a = a[:2]\n",
    "        mc_results.append(brier_score(p, a))\n",
    "    else:\n",
    "        num_results.append(np.abs(p - a))\n",
    "\n",
    "print(f\"T/F: {np.mean(tf_results)*100:.2f}, MCQ: {np.mean(mc_results)*100:.2f}, NUM: {np.mean(num_results)*100:.2f}\")\n",
    "print(f\"Combined Metric: {(np.mean(tf_results) + np.mean(mc_results) + np.mean(num_results))*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = []\n",
    "# for question in test_questions:\n",
    "#     preds.append(calibrated_random_baseline_model(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for question in test_questions:\n",
    "    preds.append(bert_model(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: predictions.pkl (deflated 61%)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('submission'):\n",
    "    os.makedirs('submission')\n",
    "\n",
    "with open(os.path.join('submission', 'predictions.pkl'), 'wb') as f:\n",
    "    pickle.dump(preds, f, protocol=2)\n",
    "\n",
    "!cd submission && zip ../submission.zip ./* && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                          predictions.pkl\n",
      "autocast_competition_test_set.json \u001b[34msubmission\u001b[m\u001b[m\n",
      "autocast_questions.json            submission.zip\n",
      "autocast_test_set_w_answers.csv    submission1.ipynb\n",
      "evaluate.ipynb                     submission2.ipynb\n",
      "example_submission.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "90e47ca3de7a6ac5652c507781a9a883127089d6067d2cae315ebae4b66e7ceb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
