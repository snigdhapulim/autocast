{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('data/autocast_competition_test_set.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(probabilities, answer_idx):\n",
    "    answer = np.zeros_like(probabilities)\n",
    "    answer[answer_idx] = 1\n",
    "    return ((probabilities - answer) ** 2).sum() / 2\n",
    "\n",
    "def calib_err(confidence, correct, p='2', beta=50):\n",
    "    # beta is target bin size\n",
    "    confidence = np.array(confidence)\n",
    "    correct = np.array(correct)\n",
    "    idxs = np.argsort(confidence)\n",
    "    confidence = confidence[idxs]\n",
    "    correct = correct[idxs]\n",
    "    bins = [[i * beta, (i + 1) * beta] for i in range(len(confidence) // beta)]\n",
    "    bins[-1] = [bins[-1][0], len(confidence)]\n",
    "\n",
    "    cerr = 0\n",
    "    total_examples = len(confidence)\n",
    "    for i in range(len(bins) - 1):\n",
    "        bin_confidence = confidence[bins[i][0]:bins[i][1]]\n",
    "        bin_correct = correct[bins[i][0]:bins[i][1]]\n",
    "        num_examples_in_bin = len(bin_confidence)\n",
    "\n",
    "        if num_examples_in_bin > 0:\n",
    "            difference = np.abs(np.nanmean(bin_confidence) - np.nanmean(bin_correct))\n",
    "\n",
    "            if p == '2':\n",
    "                cerr += num_examples_in_bin / total_examples * np.square(difference)\n",
    "            elif p == '1':\n",
    "                cerr += num_examples_in_bin / total_examples * difference\n",
    "            elif p == 'infty' or p == 'infinity' or p == 'max':\n",
    "                cerr = np.maximum(cerr, difference)\n",
    "            else:\n",
    "                assert False, \"p must be '1', '2', or 'infty'\"\n",
    "\n",
    "    if p == '2':\n",
    "        cerr = np.sqrt(cerr)\n",
    "\n",
    "    return cerr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoint/t5_large_top10_linear_wdecay1e-2_lr5e-5_bs8_ep10_retrbm25ce/results_epoch10.obj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m folder \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mt5_large_top10_linear_wdecay1e-2_lr5e-5_bs8_ep10_retrbm25ce\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      2\u001b[0m result_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcheckpoint/\u001b[39m\u001b[39m{\u001b[39;00mfolder\u001b[39m}\u001b[39;00m\u001b[39m/results_epoch10.obj\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m raw_logits \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39;49m(result_file, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint/t5_large_top10_linear_wdecay1e-2_lr5e-5_bs8_ep10_retrbm25ce/results_epoch10.obj'"
     ]
    }
   ],
   "source": [
    "folder = 't5_large_top10_linear_wdecay1e-2_lr5e-5_bs8_ep10_retrbm25ce'\n",
    "result_file = f'checkpoint/{folder}/results_epoch10.obj'\n",
    "\n",
    "raw_logits = pickle.load(open(result_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2logits = {}\n",
    "for logits in raw_logits:\n",
    "    id, answer, logits = logits\n",
    "    id2logits[id + answer] = logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf, mc, reg = [],[],[]\n",
    "reg_answers = []\n",
    "tf_brier, mc_brier = [],[]\n",
    "tf_conf, tf_correct, mc_conf, mc_correct = [],[],[],[]\n",
    "\n",
    "assert len(data) == len(raw_logits)\n",
    "\n",
    "for i, (obj, logits) in enumerate(zip(data, raw_logits)):\n",
    "    # id, logits = logits\n",
    "    logits = id2logits[obj['question_id'] + str(obj['answers'][0])]\n",
    "\n",
    "    if obj['answers'][0] in ['yes', 'no']:\n",
    "        assert len(logits) == 2, (len(logits),i)\n",
    "        probabilities = softmax(logits)\n",
    "        answer_idx = ['no', 'yes'].index(obj['answers'][0])\n",
    "        \n",
    "        tf_conf.append(probabilities.max())\n",
    "        tf_correct.append(probabilities.argmax() == answer_idx)\n",
    "        tf_brier.append(brier_score(probabilities, answer_idx))\n",
    "\n",
    "    elif type(obj['choices']) is dict:\n",
    "        assert len(logits) == 1, len(logits)\n",
    "\n",
    "        answer = float(obj['answers'][0])\n",
    "        \n",
    "        reg.append(np.abs(logits[0] - answer))\n",
    "        reg_answers.append(logits[0])\n",
    "\n",
    "    else:\n",
    "        assert len(logits) == 12, len(logits)\n",
    "        probabilities = softmax(logits)\n",
    "        answer_idx = ord(obj['answers'][0]) - ord('A')\n",
    "\n",
    "        mc_conf.append(probabilities.max())\n",
    "        mc_correct.append(probabilities.argmax() == answer_idx)\n",
    "        mc_brier.append(brier_score(probabilities, answer_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.mean(tf_correct)*100:.2f}, {np.mean(mc_correct)*100:.2f}, {np.mean(reg)*100:.2f}\")\n",
    "print(f\"{(np.mean(tf_correct) + np.mean(mc_correct) - np.mean(reg)) * 50:.2f}\")\n",
    "print(calib_err(tf_conf, tf_correct), calib_err(mc_conf, mc_correct))\n",
    "print(f\"{calib_err(tf_conf + mc_conf, tf_correct + mc_correct):.2f}\")\n",
    "print(f\"{np.mean(tf_brier)*100:.2f}, {np.mean(mc_brier)*100:.2f}\")\n",
    "print(f\"{(np.mean(tf_brier) + np.mean(mc_brier) + np.mean(reg)) * 100/3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'temporal_t5_large_top1_seqlen64_fixed_wdecay1e-2_lr5e-5_bs8_ep5_retrbm25ce_finetune0_adjusttarget1'\n",
    "result_file = f'checkpoint/{folder}/results_epoch5.obj'\n",
    "\n",
    "raw_logits = pickle.load(open(result_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf, mc, reg = [],[],[]\n",
    "tf_brier, mc_brier = [],[]\n",
    "tf_conf, tf_correct, mc_conf, mc_correct = [],[],[],[]\n",
    "\n",
    "assert len(data) == len(raw_logits)\n",
    "\n",
    "for obj, seq_logits in zip(data, raw_logits):\n",
    "    # seq_logits [SEQ, C]\n",
    "    logits = seq_logits[-1]\n",
    "    if obj['answers'][0] in ['yes', 'no']:\n",
    "        assert len(logits) == 2, len(logits)\n",
    "        probabilities = softmax(logits)\n",
    "        answer_idx = ['yes', 'no'].index(obj['answers'][0])\n",
    "        \n",
    "        tf_conf.append(probabilities.max())\n",
    "        tf_correct.append(probabilities.argmax() == answer_idx)\n",
    "        tf_brier.append(brier_score(probabilities, answer_idx))\n",
    "\n",
    "    elif type(obj['choices']) is dict:\n",
    "\n",
    "        answer = float(obj['answers'][0])\n",
    "        \n",
    "        reg.append(np.abs(logits - answer))\n",
    "\n",
    "    else:\n",
    "        assert len(logits) == 12, len(logits)\n",
    "        probabilities = softmax(logits)\n",
    "        answer_idx = ord(obj['answers'][0]) - ord('A')\n",
    "        \n",
    "        mc_conf.append(probabilities.max())\n",
    "        mc_correct.append(probabilities.argmax() == answer_idx)\n",
    "        mc_brier.append(brier_score(probabilities, answer_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.mean(tf_correct)*100:.2f}, {np.mean(mc_correct)*100:.2f}, {np.mean(reg)*100:.2f}\")\n",
    "print(f\"{(np.mean(tf_correct) + np.mean(mc_correct) - np.mean(reg)) * 50:.2f}\")\n",
    "print(calib_err(tf_conf, tf_correct), calib_err(mc_conf, mc_correct))\n",
    "print(f\"{calib_err(tf_conf + mc_conf, tf_correct + mc_correct):.2f}\")\n",
    "print(f\"{np.mean(tf_brier)*100:.2f}, {np.mean(mc_brier)*100:.2f}\")\n",
    "print(f\"{(np.mean(tf_brier) + np.mean(mc_brier) + np.mean(reg)) * 100/3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = '3b'\n",
    "result_file = f'../results/unifiedqa_{model_size}_results.obj'\n",
    "\n",
    "raw_logits = pickle.load(open(result_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf, mc, reg = [],[],[]\n",
    "tf_brier, mc_brier = [],[]\n",
    "tf_conf, tf_correct, mc_conf, mc_correct = [],[],[],[]\n",
    "\n",
    "assert len(data) == len(raw_logits)\n",
    "\n",
    "for obj, logits in zip(data, raw_logits):\n",
    "    if obj['answers'][0] in ['yes', 'no']:\n",
    "        assert len(logits) == 2, len(logits)\n",
    "        probabilities = softmax(logits)\n",
    "        answer_idx = ['no', 'yes'].index(obj['answers'][0])\n",
    "        \n",
    "        tf_conf.append(probabilities.max())\n",
    "        tf_correct.append(probabilities.argmax() == answer_idx)\n",
    "        tf_brier.append(brier_score(probabilities, answer_idx))\n",
    "\n",
    "    elif type(obj['choices']) is dict:\n",
    "        assert len(logits) == 0, len(logits)\n",
    "\n",
    "        answer = float(obj['answers'][0])\n",
    "        reg.append(np.abs(np.random.uniform() - answer))\n",
    "\n",
    "    else:\n",
    "        # assert len(logits) == 12, len(logits)\n",
    "        probabilities = softmax(logits)\n",
    "        answer_idx = ord(obj['answers'][0]) - ord('A')\n",
    "\n",
    "        mc_conf.append(probabilities.max())\n",
    "        mc_correct.append(probabilities.argmax() == answer_idx)\n",
    "        mc_brier.append(brier_score(probabilities, answer_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.mean(tf_correct)*100:.2f}, {np.mean(mc_correct)*100:.2f}, {np.mean(reg)*100:.2f}\")\n",
    "print(f\"{(np.mean(tf_correct) + np.mean(mc_correct) - np.mean(reg)) * 50:.2f}\")\n",
    "print(calib_err(tf_conf, tf_correct), calib_err(mc_conf, mc_correct))\n",
    "print(f\"{calib_err(tf_conf + mc_conf, tf_correct + mc_correct):.2f}\")\n",
    "print(f\"{np.mean(tf_brier)*100:.2f}, {np.mean(mc_brier)*100:.2f}\")\n",
    "print(f\"{(np.mean(tf_brier) + np.mean(mc_brier) + np.mean(reg)) * 100/3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get crowd metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf, mc, reg = [],[],[]\n",
    "tf_brier, mc_brier = [],[]\n",
    "tf_conf, tf_correct, mc_conf, mc_correct = [],[],[],[]\n",
    "temp = []\n",
    "\n",
    "for obj in data:\n",
    "    if obj['answers'][0] in ['yes', 'no']:\n",
    "        answer_idx = ['yes', 'no'].index(obj['answers'][0])\n",
    "\n",
    "        p = float(obj['targets'][-1]['target'])\n",
    "\n",
    "        probabilities = np.array([1-p, p])\n",
    "        \n",
    "        tf_conf.append(probabilities.max())\n",
    "        tf_correct.append(probabilities.argmax() == answer_idx)\n",
    "        tf_brier.append(brier_score(probabilities, answer_idx))\n",
    "\n",
    "    elif type(obj['choices']) is dict:\n",
    "\n",
    "        answer = float(obj['answers'][0])\n",
    "        p = float(obj['targets'][-1]['target'])\n",
    "        p = min(p, 1)\n",
    "        p = max(p, 0)\n",
    "        \n",
    "        reg.append(np.abs(p - answer))\n",
    "\n",
    "    else:\n",
    "        answer_idx = ord(obj['answers'][0]) - ord('A')\n",
    "        \n",
    "        probabilities = np.array([float(p) for p in obj['targets'][-1]['target'][:len(obj['choices'])]])\n",
    "        # print(obj['question'], probabilities, obj['answers'][0])\n",
    "        # if obj['answers'][0] == 'D': break\n",
    "\n",
    "        mc_conf.append(probabilities.max())\n",
    "        mc_correct.append(probabilities.argmax() == answer_idx)\n",
    "        mc_brier.append(brier_score(probabilities, answer_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.mean(tf_correct)*100:.2f}, {np.mean(mc_correct)*100:.2f}, {np.mean(reg)*100:.2f}\")\n",
    "print(f\"{(np.mean(tf_correct) + np.mean(mc_correct) - np.mean(reg)) * 50:.2f}\")\n",
    "print(calib_err(tf_conf, tf_correct), calib_err(mc_conf, mc_correct))\n",
    "print(f\"{calib_err(tf_conf + mc_conf, tf_correct + mc_correct):.2f}\")\n",
    "print(f\"{np.mean(tf_brier)*100:.2f}, {np.mean(mc_brier)*100:.2f}\")\n",
    "print(f\"{(np.mean(tf_brier) + np.mean(mc_brier) + np.mean(reg)) * 100/3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf, mc, reg = [],[],[]\n",
    "tf_brier, mc_brier = [],[]\n",
    "tf_conf, tf_correct, mc_conf, mc_correct = [],[],[],[]\n",
    "temp = []\n",
    "\n",
    "for obj in data:\n",
    "    if obj['answers'][0] in ['yes', 'no']:\n",
    "        answer_idx = ['yes', 'no'].index(obj['answers'][0])\n",
    "\n",
    "        p = np.random.random()\n",
    "\n",
    "        probabilities = np.array([1-p, p])\n",
    "        \n",
    "        tf_conf.append(probabilities.max())\n",
    "        tf_correct.append(probabilities.argmax() == answer_idx)\n",
    "        tf_brier.append(brier_score(probabilities, answer_idx))\n",
    "\n",
    "    elif type(obj['choices']) is dict:\n",
    "\n",
    "        answer = float(obj['answers'][0])\n",
    "\n",
    "        p = np.random.random()\n",
    "        p = min(p, 1)\n",
    "        p = max(p, 0)\n",
    "        \n",
    "        reg.append(np.abs(p - answer))\n",
    "\n",
    "    else:\n",
    "        answer_idx = ord(obj['answers'][0]) - ord('A')\n",
    "        \n",
    "        probabilities = np.random.uniform(size=len(obj['choices']))\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        mc_conf.append(probabilities.max())\n",
    "        mc_correct.append(probabilities.argmax() == answer_idx)\n",
    "        mc_brier.append(brier_score(probabilities, answer_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.mean(tf_correct)*100:.2f}, {np.mean(mc_correct)*100:.2f}, {np.mean(reg)*100:.2f}\")\n",
    "print(f\"{(np.mean(tf_correct) + np.mean(mc_correct) - np.mean(reg)) * 50:.2f}\")\n",
    "print(calib_err(tf_conf, tf_correct), calib_err(mc_conf, mc_correct))\n",
    "print(f\"{calib_err(tf_conf + mc_conf, tf_correct + mc_correct):.2f}\")\n",
    "print(f\"{np.mean(tf_brier)*100:.2f}, {np.mean(mc_brier)*100:.2f}\")\n",
    "print(f\"{(np.mean(tf_brier) + np.mean(mc_brier)) * 100/2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65c0d127d4104eab02b28ce0e894db0584499721c6d7f16c3cf8420bcd2e03b0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('forecasting')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
